{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science: Bridging Principle and Practice\n",
    "## Part 7: Linear Regression Model (Bike Sharing case study)\n",
    "\n",
    "<br/>\n",
    "\n",
    "<div class=\"container\">\n",
    "    <div style=\"float:left;width:40%\">\n",
    "\t    <img src=\"images/bikeshare_sun.jpg\">\n",
    "    </div>\n",
    "    <div style=\"float:left;width:40%\">\n",
    "\t    <img src=\"images/bikeshare_snow.PNG\">\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "<ol start=\"7\">\n",
    "    <li><a href=\"section 7\">Linear Regression Model</a>\n",
    "        <ol type=a>\n",
    "            <br>\n",
    "            <li><a href=\"section7a\">Explanatory and Response Variables</a></li>\n",
    "            <br>\n",
    "            <li><a href=\"section7b\">Finding &beta;</a></li>\n",
    "            <br>\n",
    "            <li><a href=\"section7c\">Evaluating the Model</a></li>            \n",
    "        </ol>\n",
    "    </li>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to import some necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from scipy.linalg import lstsq\n",
    "\n",
    "def make_X(df, var_names):\n",
    "    \"\"\"Given a DataFrame and a list of explanatory variables, one-hot encodes\n",
    "    variables if they are categorical and returns a dataframe with \n",
    "    all the given explanatory variables.\"\"\"\n",
    "    data = df.loc[:, var_names]\n",
    "    categorical = [\"month\", \"week day\", \"season\"]\n",
    "    boolean = [\"is holiday\", \"is work day\"]\n",
    "    X = pd.DataFrame({\"intercept\":np.ones(df.shape[0], dtype='int')})\n",
    "    for var in data.columns:\n",
    "        if var in categorical:\n",
    "            dummies = pd.get_dummies(data[var])\n",
    "            formatted = dummies.drop(dummies.columns[-1], axis=1)\n",
    "        elif var in boolean:\n",
    "            formatted = (data[var] == \"yes\") * 1\n",
    "        else:\n",
    "            formatted = data.loc[:, [var]]\n",
    "        X = X.join(formatted)\n",
    "      \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study: Capital Bike Share <a id= \"sectioncase\"></a>\n",
    "\n",
    "Bike-sharing systems have become increasingly popular worldwide as environmentally-friendly solutions to traffic congestion, inadequate public transit, and the \"last-mile\" problem. Capital Bikeshare runs one such system in the Washington, D.C. metropolitan area.\n",
    "\n",
    "The Capital Bikeshare system comprises docks of bikes, strategically placed across the area, that can be unlocked by *registered* users who have signed up for a monthly or yearly plan or by *casual* users who pay by the hour or day. They collect data on the number of casual and registered users per hour and per day.\n",
    "\n",
    "Let's say that Capital Bikeshare is interested in a **prediction** problem: predicting how many riders they can expect to have on a given day. [UC Irvine's Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset) has combined the bike sharing data with information about weather conditions and holidays to try to answer this question.\n",
    "\n",
    "In this notebook, we'll walk through the steps a data scientist would take to answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to load the data\n",
    "bike_train = pd.read_csv(\"../resource/data/day_renamed_dso.csv\", )\n",
    "\n",
    "# load the test data\n",
    "bike_test = pd.read_csv(\"../resource/data/day_test.csv\")\n",
    "\n",
    "# reformat the date column from strings to dates\n",
    "bike_train['date'] = pd.to_datetime(bike_train['date'])\n",
    "bike_test['date'] = pd.to_datetime(bike_test['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. The Regression Model <a id= \"section7\"></a>\n",
    "\n",
    "To try to predict the number of riders on a given day, we'll use a regression model. \n",
    "\n",
    "> A **simple regression model** describes how the conditional mean of a response variable $y$ depends on an explanatory variable $x$: $$\\mu_{y|x} = \\beta_0 + \\beta_1x$$ This equation describes our best guess for $y$ given a particular  $x$.\n",
    "\n",
    "> A **multiple regression model** describes how the conditional mean of a response variable $y$ depends on multiple explanatory variables $x_1, x_2, ..., x_k$: $$\\mu_{y|x_1, x_2, ..., x_k} = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_kx_k$$ This equation describes our best guess for $y$ given a particular vector $x_1, x_2, ..., x_k$.\n",
    "\n",
    "In this case, our model will look something like this:\n",
    "$$\\mu_{\\text{rider count}|\\text{temp, humidity, ..., month}} = \\beta_0 + \\beta_1*\\text{(temp)} + \\beta_2*\\text{(humidity)} + ... + \\beta_k*\\text{(month)}$$\n",
    "The code for either a simple or multiple regression model is basically identical except for the number of columns we select for inclusion in our explanatory variables.\n",
    "\n",
    "To create our model, we need to:\n",
    "1. isolate the explanatory variables (X) and response variable (y) we want to use\n",
    "2. find the values for the $\\beta$ variables on the best-fit regression line\n",
    "3. evaluate the accuracy of our model\n",
    "\n",
    "### 7a. Explanatory and response variables <a id=\"subsection7a\"></a>\n",
    "\n",
    "First, let's decide on our response variable. We'll try to predict the *total number of riders* on a given day. The response variable needs to be in an array (not a DataFrame), so we'll get it using indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response variable: total count of riders in a day (training set)\n",
    "y_train = bike_train['total riders']\n",
    "\n",
    "# response variable: total count of riders in a day (validation set)\n",
    "y_test = bike_test['total riders']\n",
    "\n",
    "# show the first 5 items in the array\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to choose our explanatory variables. Let's try predicting ridership in terms of _temperature_, _work day_, and _season_.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>Why don't we just use all the explanatory variables?</b>\n",
    "<p>\n",
    "    You might think that the best model would use <i>all</i> the available explanatory information. But, using many variables makes a model <b>computationally expensive</b>. In the real world, where data sets may have a million or more rows, using a complex model can increase the time and computing power needed to make preditions. Additionally, many variables may have <b>little predictive power</b> such that excluding them from the model doesn't lower the accuracy very much. Other variables might <b>add noise</b> that actually decreases the model's performance outside of the training data.\n",
    "    </p>\n",
    "\n",
    "</div> \n",
    "\n",
    "Here, we run into a problem: our DataFrame has several categorical variables, including \"season\" and \"week day\". Categorical variables seem like they might be helpful in predicting the number of riders; however, we can't feed text into a mathematical model. And, if we code them as integers, as they were coded in the original data set (e.g. Monday = 1, Tuesday = 2, ...) that can lead to questionable manipulations. For example, if Sunday is coded as 0 and Saturday is coded as 6, the computer might conclude that the average of Sunday and Saturday is Wednesday (since Wednesday is coded as 3).\n",
    "\n",
    "#### One-Hot Encoding\n",
    "To work around this, we will **one-hot encode** all our categorical variables. In one-hot encoding, the possible values of the variable each get their own column of 1s and 0s. The value is a 1 if that day falls in that category and a 0 otherwise.\n",
    "\n",
    "Here's an example. Say we have three possible weather states: rain, cloudy, or sunny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example DataFrame of weather states\n",
    "categorical = pd.DataFrame({\"weather\": [\"rainy\", \"cloudy\", \"sunny\", \"cloudy\"]})\n",
    "categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one-hot encoding would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.get_dummies is a function that does dummy encoding\n",
    "one_hot = pd.get_dummies(categorical['weather'])\n",
    "one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in each row, only one of the values is equal to 1 (hence the name, \"one-hot\" encoding), since no day can have more than one weather state.\n",
    "\n",
    "Notice also that we don't technically need the third column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot.drop(\"sunny\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we know that there are only three possible weather states, and we see that day 2 was neither cloudy nor rainy (that is, `cloudy`=0 and `rainy`=0), day 2 *must* have been sunny. This is helpful to save computation time and space. If you have some knowledge of linear algebra, note that this is also helpful to solve the problem of *perfect multicollinearity*- a situation that can make it impossible to compute the optimal set of $\\beta$s.\n",
    "\n",
    "For simplicity, we've provided a function called `make_X` that will convert our data to the correct format for prediction, including one-hot encoding the categorical variables. `make_X` takes two arguments:\n",
    "* the first argument, `df`, is the DataFrame with the data we want to use to make predictions\n",
    "* the second argument, `var_names`, is a list with the names of all the explanatory variables in `df` we want our model to use to make predictions: categorical and numerical\n",
    "\n",
    "`make_X` will also add a column called \"intercept\" that only contains 1s. This column will help find the intercept term $\\beta_0$ in our regression line. You can think of the intercept term as an $x_0$ that gets multiplied by $\\beta_0$ and is always equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the explanatory variables to use in a DataFrame\n",
    "expl_vars = [\"temp\", \"is work day\", \"season\"]\n",
    "\n",
    "# convert the explanatory table to the correct format\n",
    "X_train = make_X(bike_train, expl_vars)\n",
    "\n",
    "# show the first five rows of the model inputs\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>EXERCISE:</b> \n",
    "    <p>\n",
    "        Since we want to try the model on the test data as well, we will also perform the same transformations on the test set so it can fit the model. Fill in the code to first select the explanatory variables \"temp\", \"is work day\", and \"season\", then convert the explanatory table to the matrix format using <code>format_X</code>. \n",
    "    </p>\n",
    "    <p>Hint: we'll need to go through the exact same steps as in the above cell for the training data, but any references to training data should be replaced by their test data counterparts.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the explanatory variables to use in a Table\n",
    "expl_vars = ...\n",
    "\n",
    "# convert the explanatory table to the correct format\n",
    "X_test = ...\n",
    "\n",
    "# show the first five rows of the model inputs\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7b.  Finding $\\beta$ <a id=\"subsection7b\"></a>\n",
    "The next step is to calculate the $\\beta$ terms. We can do this with a function from the [Scipy statistical analysis Python package](https://www.scipy.org/) called `lstsq`.\n",
    "\n",
    "Given a matrix of explanatory variables X and an array of response variables y, `lstsq` returns a vector $\\beta$. `lstsq` uses *ordinary least squares* as its **loss function (L)**: the function that defines the training loss (error) and what we seek to minimize (often using linear algebra or calculus, depending on the loss function). The ordinary least squares equation is a common loss function that is used to minimize the sum of squared errors:\n",
    "\n",
    "$$L(\\beta) = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\text{predicted}(\\beta, x_i))^2$$\n",
    "\n",
    "where $n$ is the number of days, $y_i$ is the actual number of riders on the $i$th day, and $\\text{predicted}(\\beta, x_i)$ is number of riders predicted to be on the $i$th day when using $\\beta$ and the explanatory variables $x_i$. When minimized, the loss function will yield our optimal $\\beta$. \n",
    "\n",
    "`lstsq` returns a list of four things, but for our purposes we're only interested in one of them: the array of the $\\beta$ values for the best-fit line. If you're curious about exactly how `lstsq` works, you can [read the docs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lstsq.html) for more information, but that isn't necessary to complete this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the least squares solution\n",
    "lstsq_results = lstsq(X_train, y_train)\n",
    "\n",
    "beta = lstsq_results[0]\n",
    "\n",
    "beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have everything we need to make predictions about the total number of riders on a given day. Remember, the formula is: $$\\mu_{y|x_1, x_2, ..., x_k} = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_kx_k$$\n",
    "\n",
    "So, to make a prediction for a given day, we take all the values in the X matrix corresponding to that day, multiply each value by its corresponding $\\beta$, then add them all together. The `@` operator can help us with this matrix multiplication.\n",
    "\n",
    "For example, here's the first row of our explanatory variable matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.loc[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the prediction, we use `@` to multiply each item in the row by each corresponding item in the $\\beta$ vector and sum them all up. If you've taken linear algebra, you'll recognize this as the [*dot product*](https://en.wikipedia.org/wiki/Dot_product).\n",
    "\n",
    "<img src=\"images/vector_mult.png\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiply the arrays to get the prediction\n",
    "X_train.loc[0, :] @ beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `@` operator can also work on matrices. To get the predictions for *every* row in X, we use exactly the same syntax.\n",
    "\n",
    "<img src=\"images/matrix_mult.png\"  />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the dot product of every row of training data and beta\n",
    "# the resulting array is a predicted number of total riders for each day\n",
    "predict_train = X_train @ beta\n",
    "\n",
    "# show the first 5 predictions\n",
    "predict_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add our predictions to our original table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new column in our training data with the predicted total riders\n",
    "bike_train[\"predicted total riders\"] = predict_train\n",
    "\n",
    "# show the first five rows of the DataFrame\n",
    "bike_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>EXERCISE:</b> We also want to make predictions for the test data using the $\\beta$ we found during training. Replace the <code>...</code> in the cell below with an expression to calculate the predictions for the test set. Remember- you need to use <code>@</code> to multiply each row of explanatory variables in our test set by the $\\beta$ vector. Look at how <code>predict_train</code> was calculated for a guide.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions for the test data using beta and the dot product\n",
    "predict_test = ...\n",
    "\n",
    "# create a new column in our test data with predicted total riders\n",
    "bike_test[\"predicted total riders\"] = predict_test\n",
    "\n",
    "# show the first 5 rows of test data\n",
    "bike_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7c. Evaluating the model <a id=\"subsection7c\"></a>\n",
    "\n",
    "Our model makes predictions, but how good are they? We can start to get a sense of how we did by plotting the predictions versus the actual values on our training data on a scatter plot. If our model predicts perfectly:\n",
    "\n",
    "- the predicted values will be equal to the actual values\n",
    "- all points in the scatter plot will fall along a straight line with a slope of 1\n",
    "\n",
    "To do this, we'll use a Seaborn function called `regplot`. `regplot` creates the scatter plot, then adds the linear regression line of best fit. The translucent band around the line represents the 95% confidence interval for the regression estimate- we'll talk more about confidence intervals in a future notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the plots bigger\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "\n",
    "# create the scatter plot and regression line\n",
    "sns.regplot(x=\"total riders\", y=\"predicted total riders\", data=bike_train);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the test set predictions scattered against the actual total rider counts. Note that we've added an extra `color` argument to `regplot` to change the color of the dots and line and distinguish the test data from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the scatter plot and regression line\n",
    "sns.regplot(x=\"total riders\", y=\"predicted total riders\", data=bike_test, color=(1.0, 200/256, 44/256));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Squared Error\n",
    "Now it would be nice to have a quantitative measure of how good our model is. An intuitive way to evaluate our model is to look at the *error* for each prediction: the size of the difference between the number of riders we predicted and the actual number of riders. We can get this for each item in our data set by **subtracting the array of predictions from the array of actual values**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an array of errors\n",
    "errors = y_train - predict_train\n",
    "\n",
    "# show the first five items in the error array\n",
    "errors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know the error for each individual prediction, but how is the model doing in general? Let's try taking the average of the errors using the `np.average` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the average of the errors\n",
    "np.average(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `e-13` is scientific notation: it means that we should take the number to the left of it and multiply it by $10^{-13}$ (0.0000000000001). The resulting number is very, very close to zero. So, if the average error is almost zero, that would seem to imply that our model is, on average, making near perfect predictions. We know this is untrue thanks to our scatter plot.\n",
    "\n",
    "What happened?\n",
    "\n",
    "The problem is that about half of our errors are negative (overestimation) and the other half are positive (underestimation). This is actually a good thing- it means that our model isn't systematically over or underestimating. However, it means that averaging the errors is not a good quantitative measurement of our model, since the negative errors cancel out the positive errors.\n",
    "\n",
    "In this case, we care about the *magnitude* of the errors: how far each prediction is from the actual value regardless of whether the prediction is over or under. To make all of our errors positive while retaining the magnitude, let's **square all of our errors**. In Python, `**` is the exponential operation; `3 ** 2` tells the computer to multiply 3 by itself 2 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# square the array of errors\n",
    "sq_error = errors ** 2\n",
    "\n",
    "# show the first five squared errors\n",
    "sq_error.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that none of our errors are negative, we can safely **take the average**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the average of the squared errors\n",
    "mean_sq_error = np.average(sq_error)\n",
    "mean_sq_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The units for the mean squared error are squared riders, which is hard to interpret in real life. Ideally, we would want an average error measure in *riders* so we could say that on average, our model is off by this many riders.\n",
    "\n",
    "Since mean squared error is in squared riders, we can get a number with a unit of riders by **taking the square root** of our mean squared error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the square root of the mean squared errors\n",
    "root_mean_sq_err = np.sqrt(mean_sq_error)\n",
    "root_mean_sq_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This number is the **Root Mean Squared Error** (RMSE, sometimes also called the Root Mean Squared Deviation). RMSE is our quantitative measure of the average performance of our model. To summarize the steps:\n",
    "\n",
    "1. Find the difference between the actual and predicted value for each of our data points (the errors)\n",
    "2. Square all of the errors\n",
    "3. Take the average of the squared errors\n",
    "4. Take the square root of the average squared error\n",
    "\n",
    "If you like formulas, here it is in a formula:\n",
    "\n",
    "$$rmse=\\sqrt{\\frac{\\sum_{i=1}^n (y_i - prediction_i)^2}{n}}$$\n",
    "\n",
    "Next, we want to see what the RMSE is for our test set predictions. \n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "    <b>EXERCISE:</b> Calculate the root mean squared error for the test set. Follow each of the above steps, and look at how it was calculated for the training set for some hints.\n",
    "</div>\n",
    "\n",
    "Before you run the next cell: would you expect the RMSE for the test set would be higher, lower, or about the same as the RMSE for the training set? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the error for each data point\n",
    "test_errors = ...\n",
    "\n",
    "# square the errors\n",
    "test_sq_error = ...\n",
    "\n",
    "# take the mean of the squared errors\n",
    "test_mean_sq_error = ...\n",
    "\n",
    "# take the square root of the mean squared errors\n",
    "test_rmse = ...\n",
    "test_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Error\n",
    "We can also visualize our errors compared to the actual values on a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training errors on a scatter plot\n",
    "bike_train[\"training error\"] = errors\n",
    "sns.regplot(x=\"predicted total riders\", y=\"training error\", data=bike_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the test errors on a scatter plot\n",
    "bike_test[\"test error\"] = y_test - predict_test\n",
    "sns.regplot(x=\"predicted total riders\", y=\"validation error\", data=bike_test, color=(1.0, 200/256, 44/256));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <b>QUESTION:</b> Based on the plots and root mean squared error above, how well do you think our model is doing? What does the shape of the scatter plot of errors tell us about the appropriateness of the linear model here?\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "- Bike-Sharing data set from University of California Irvine's Machine Learning Repository https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset\n",
    "- Portions of text and code adapted from Professor Jonathan Marshall's Legal Studies 190 (Data, Prediction, and Law) course materials: [lab 2-22-18, Linear Regression](https://github.com/ds-modules/LEGALST-190/tree/master/labs/2-22) (Author Keeley Takimoto)  and [lab 3-22-18, Exploratory Data Analysis](https://github.com/ds-modules/LEGALST-190/tree/masterlabs/3-22) (Author Keeley Takimoto)\n",
    "- \"Capital Bikeshare, Washington, DC\" header image by [Leeann Caferatta](https://www.flickr.com/photos/leeanncafferata/34309356871) licensed under [CC BY-ND 2.0](https://creativecommons.org/licenses/by-nd/2.0/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
