{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science: Bridging Principles and Practice\n",
    "## Scikit-Learn Template\n",
    "\n",
    "<img src=\"images/ml.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview <a id=\"section9b\"></a>\n",
    "\n",
    "[Scikit-Learn](https://scikit-learn.org/stable/index.html) is free, open-source software for machine learning in Python. The software includes tools to create, train, and evaluate machine learning models.\n",
    "\n",
    "Overview of using Scikit-Learn for machine learning:\n",
    "\n",
    "1. Load the data\n",
    "1. Choose your explanatory variables (what you're going to use to make predictions) and response variable (what you want to predict) \n",
    "1. Split the data into training and testing sets (and, if you're doing a lot of parameter tuning, a validation set)\n",
    "2. Create a model in Python\n",
    "3. Fit the data to the model\n",
    "4. Make predictions using your fitted model\n",
    "5. Score the accuracy of your model\n",
    "8. Visualize your results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you use this template\n",
    "- This template assumes that your dataset is already clean.\n",
    "- You may choose to clean the data using Python or another software, like Excel. For tools to clean data using Python, please see the Data-Cleaning-template notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to import some necessary software\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# set the random seed for reproducibility\n",
    "np.random.seed(28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Load the clean data\n",
    "\n",
    "\n",
    "- If you're working in the Haas Executive Education cloud server, you will need to upload your dataset. Go to [bee.haas.berkeley.edu](https://bee.haas.berkeley.edu). Click on the \"haas-ds-online\" folder, then the \"data\" folder. Then, click the \"Upload\" button at the top right. The relative path to your dataset is now \"data/name-of-your-file\"\n",
    "- If you're on your computer, make sure you know the relative path of your data file. Putting it in the same directory as your notebook will help.\n",
    "- Replace the ... with the relative path of your data file. Don't forget the file extension.\n",
    "- Use the first code cell if your data is in a csv (comma separated values) file.  Use the second code cell if your data is in an Excel file.\n",
    "- If your data is in a different file type, you can see if there are functions to read it with Pandas at [this link](https://pandas.pydata.org/docs/user_guide/io.html). Note that csv and Excel files tend to be the easiest to work with in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to load the data\n",
    "data = pd.read_csv(...)\n",
    "\n",
    "# show the first 5 rows of the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  For Excel file data\n",
    "data = pd.read_csv(...)\n",
    "\n",
    "# show the first 5 rows of the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Select the explanatory and response variables\n",
    "- explanatory variables should be the names of the appropriate columns, each enclosed in quotation marks, listed inside the square brackets and separated by commas \n",
    "- response variable should be the name of the appropriate column, enclosed in quotation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose explanatory and response variables\n",
    "expl_vars = [...]\n",
    "\n",
    "resp_var = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the X DataFrame\n",
    "X = data.loc[:, expl_vars]\n",
    "\n",
    "# show the first 5 rows\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the y array\n",
    "y = data[resp_var]\n",
    "\n",
    "# show the first 5 items\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Split into train, test and (optionally) validation sets\n",
    "\n",
    "- the random seed can be any number, as long as it's consistent\n",
    "- use a validation set if you want to go through the full model selection process, including tuning hyperparameters. See Notebook 08 (Model Selection) for an example.\n",
    "- running only the first cell will put 80% of the data in the training set and 20% in the test set\n",
    "- running the first and second cells will put 60% of the data in the training set, 20% in the test set, and 20% in the validation set\n",
    "- to change the proportions of how much data goes in each set, edit the train_size and test_size arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the random seed\n",
    "np.random.seed(28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# run this cell to split the data into training and test sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to create a validation set, delete the # from the beginning of the next line and run the cell\n",
    "\n",
    "# X_train, X_test, X_val, y_val = train_test_split(X, y, train_size=0.75, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Import and create the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the code that creates linear regression models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# create a new, untrained model\n",
    "lr_model = LinearRegression(fit_intercept=True, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Make predictions with fitted model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the predictions to a variable\n",
    "lr_train_predictions = lr_model.predict(X_train)\n",
    "# show the predictions\n",
    "lr_train_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the validation set (if you're not using a validation set, this will error):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're using a validation set:\n",
    "# save the predictions to a variable\n",
    "lr_val_predictions = lr_model.predict(X_val)\n",
    "# show the predictions\n",
    "lr_val_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the predictions to a variable\n",
    "lr_test_predictions = lr_model.predict(X_test)\n",
    "# show the predictions\n",
    "lr_test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Score the model\n",
    "\n",
    "- note: depending on which algorithm you use, the `score` method will return something slightly different. The [Scikit-Learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)for each type of model will have details on what `score` shows if you scroll down.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the score to a variable\n",
    "lr_train_score = lr_model.score(X_train, y_train)\n",
    "\n",
    "# show the score\n",
    "lr_train_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the score to a variable\n",
    "lr_val_score = lr_model.score(X_val, y_val)\n",
    "\n",
    "# show the score\n",
    "lr_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the score to a variable\n",
    "lr_score = lr_model.score(X_test, y_test)\n",
    "\n",
    "# show the score\n",
    "lr_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Results\n",
    "\n",
    "The following cell shows code to plot predicted vs actual values for the training data. The cell after that has the same code with references to the training data removed, if you want to plot validation or test data.\n",
    "\n",
    "Note: scatter plots work well for problems that work with continuous, numerical data, like regression problems. Other types of visualizations may be appropriate for other problems. Check out the [Seaborn Python visualization library](https://seaborn.pydata.org/tutorial.html) for example code for other types of plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the blank subplots and increase their size \n",
    "f, [ax1, ax2] = plt.subplots(2, figsize=(12, 12))\n",
    "\n",
    "# plot the actual values (x) against the predicted values (y)\n",
    "sns.regplot(x=y_train, y=lr_train_predictions, ax=ax1, color=\"#003262\") \n",
    "ax1.set_xlabel(\"actual\")\n",
    "ax1.set_ylabel(\"predicted\")\n",
    "ax1.set_title(\"Predicted vs. Actual Values\")\n",
    "\n",
    "# plot the actual values (x) against the error for each prediction (y)\n",
    "sns.scatterplot(x=y_train, y=y_train - lr_train_predictions, ax=ax2, color=\"#FDB515\") \n",
    "ax2.set_xlabel(\"actual\")\n",
    "ax2.set_ylabel(\"error\")\n",
    "ax2.set_title(\"Error\")\n",
    "ax2.hlines(y=0, xmin=min(y_test), xmax=max(y_test));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the blank subplots and increase their size \n",
    "f, [ax1, ax2] = plt.subplots(2, figsize=(12, 12))\n",
    "\n",
    "# plot the actual values (x) against the predicted values (y)\n",
    "sns.regplot(x=..., y=..., ax=ax1, color=\"#003262\") \n",
    "ax1.set_xlabel(\"actual\")\n",
    "ax1.set_ylabel(\"predicted\")\n",
    "ax1.set_title(\"Predicted vs. Actual Values\")\n",
    "\n",
    "# plot the actual values (x) against the error for each prediction (y)\n",
    "sns.scatterplot(x=..., y=... - ..., ax=ax2, color=\"#FDB515\") \n",
    "ax2.set_xlabel(\"actual\")\n",
    "ax2.set_ylabel(\"error\")\n",
    "ax2.set_title(\"Error\")\n",
    "ax2.hlines(y=0, xmin=min(...), xmax=max(...));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a different kind of model\n",
    "\n",
    "Scikit-Learn's [\"Choosing the right estimator\" flowchart](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) is a nice tool, but ultimately your algorithm choice should draw strongly from your knowledge of the domain, the problem, and the available algorithms.\n",
    "\n",
    "You can find the [list of supervised and unsupervised learning algorithms in Scikit-Learn here](https://scikit-learn.org/stable/user_guide.html). To use a different algorithm, click through the link to the algorithm you want; on each page, you'll find example code of how to use it. For most algorithms, the below code will work as long as you fill in the import statement (saying what model to import and where it's located) and fill in the code to create a new, untrained instance of that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first five rows of the attrition data\n",
    "\n",
    "from ... import ...\n",
    "\n",
    "model = ...\n",
    "model.fit(X_train, y_train)\n",
    "model.predict(X_test)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "- header image credit: \"Machine Learning & Artificial Intelligence\", [Mike Mackenzie](https://www.flickr.com/photos/mikemacmarketing/42271822770). [CC BY 2.0](https://creativecommons.org/licenses/by/2.0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook author: Keeley Takimoto"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
