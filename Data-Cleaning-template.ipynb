{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science: Bridging Principles and Practice\n",
    "## Data Cleaning Template\n",
    "\n",
    "<img src=\"images/office-and-workers-in-barcelona-spain.jpg\" />\n",
    "\n",
    "<br>\n",
    "\n",
    "*In this notebook, we will walk through solving a classification problem using machine learning. To do so, we will introduce the Scikit-Learn machine learning library for Python.*\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "<a href=\"#sectioncase\">Case Study: Employee Attrition at IBM</a>\n",
    "\n",
    "<ol start=\"9\">\n",
    "    <li><a href=\"#section9\">Machine Learning</a>\n",
    "        <ol type=a>\n",
    "            <br>\n",
    "            <li><a href=\"#section9a\">The K-Nearest Neighbors Algorithm</a></li>\n",
    "            <br>\n",
    "            <li><a href=\"#section9b\">Using Scikit-Learn: An Example</a></li>\n",
    "            <br>\n",
    "            <li><a href=\"#section9c\">Using Scikit-Learn: KNN</a></li>            \n",
    "        </ol>\n",
    "    </li>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to import some necessary software\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "\n",
    "# set the random seed for reproducibility\n",
    "np.random.seed(28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview <a id=\"section9b\"></a>\n",
    "\n",
    "This template is designed to provide helpful starter code and common steps for cleaning datasets in preparation to work with them using the [Scikit-Learn](https://scikit-learn.org/stable/index.html) library. The tools and methods in this notebook will work for many, but not all, datasets.\n",
    "\n",
    "You will get the most out of this notebook if you have already complete the 11 curriculum notebooks, or if you already have a basic familiarity with Python and Pandas.\n",
    "\n",
    "Topics for this notebook include:\n",
    "1. Loading messy data files\n",
    "2. Looking at data types, missing values, and distributions\n",
    "3. Handling missing values\n",
    "4. Performing other common tasks: unit conversion, feature engineering, one-hot encoding\n",
    "5. Saving clean dataset to a file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you use this template\n",
    "- Every dataset will have different cleaning needs. This template attempts to provide starter code for some common tasks, but it is far from comprehensive.\n",
    "- Data cleaning can be done using many non-Python tools, such as Excel or R.\n",
    "- Generally, any variables in the dataset that will go into a Scikit-Learn model should be numerical and free from missing values\n",
    "- Dataset cleaning must be considered in the context of the domain of study, the data collection method, and the problem to be solved. How the data is cleaned will depend on all these things and more.\n",
    "- Often, there isn't one single \"correct\" way to clean a particular data set. The most important thing is to keep a copy of the \"messy\" data for reference, and to clearly document all of the data cleaning choices you made as well as why you made them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Load the messy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "# fill in the ... with the path to the data file. Don't forget the file extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to load the data\n",
    "data = pd.read_csv(\"data/boston.csv\", index_col=0)\n",
    "\n",
    "# show the first 5 rows of the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Look at data types, missing values, distributions\n",
    "- explanatory variables should be the names of the appropriate columns, each enclosed in quotation marks, listed inside the square brackets and separated by commas \n",
    "- response variable should be the name of the appropriate column, enclosed in quotation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hist(figsize=(14,10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Handle missing values\n",
    "\n",
    "- the random seed can be any number, as long as it's consistent\n",
    "- use a validation set if you want to go through the full model selection process, including tuning hyperparameters. See Notebook 08 (Model Selection) for an example.\n",
    "- running only the first cell will put 80% of the data in the training set and 20% in the test set\n",
    "- running the first and second cells will put 60% of the data in the training set, 20% in the test set, and 20% in the validation set\n",
    "- to change the proportions of how much data goes in each set, edit the train_size and test_size arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Perform other dataset-specific cleaning tasks\n",
    "\n",
    "This might include:\n",
    "- using array operations to convert units\n",
    "- feature engineering: creating new columns of numerical data from text data (or other numerical data)\n",
    "- dropping irrelevant rows or columns\n",
    "\n",
    "The [Official Pandas Library Cheat Sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf) has a list of common operations as well as what they do. You can also look up details on Pandas functions by searching the [documentation](https://pandas.pydata.org/docs/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the code that creates linear regression models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# create a new, untrained model\n",
    "lr_model = LinearRegression(fit_intercept=True, normalize=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save the cleaned dataset to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "- [IBM HR Analytics Employee Attrition & Performance mock data set](https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset/home) is made available under the [Open Database License](http://opendatacommons.org/licenses/odbl/1.0/). Any rights in individual contents of the database are licensed under the [Database Contents License](http://opendatacommons.org/licenses/dbcl/1.0/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
