{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science: Bridging Principles and Practice\n",
    "## Data Cleaning Template\n",
    "\n",
    "<img src=\"images/cleaning02.jpg\" width=\"400\">\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview <a id=\"section9b\"></a>\n",
    "\n",
    "This template is designed to provide helpful starter code and common steps for cleaning datasets in preparation to work with them using the [Scikit-Learn](https://scikit-learn.org/stable/index.html) library. The tools and methods in this notebook will work for many, but not all, datasets.\n",
    "\n",
    "You will get the most out of this notebook if you have already complete the 11 curriculum notebooks, or if you already have a basic familiarity with Python and Pandas.\n",
    "\n",
    "Topics for this notebook include:\n",
    "1. Loading messy data files\n",
    "2. Looking at data types, missing values, and distributions\n",
    "3. Handling missing values\n",
    "4. Performing other common tasks: unit conversion, feature engineering, one-hot encoding\n",
    "5. Saving clean dataset to a file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you use this template, please note:\n",
    "- Every dataset will have different cleaning needs. This template attempts to provide starter code for some common tasks, but it is far from comprehensive.\n",
    "- Data cleaning can be done using many non-Python tools, such as Excel or R.\n",
    "- Generally, any variables in the dataset that will go into a Scikit-Learn model should be numerical and free from missing values\n",
    "- Dataset cleaning must be considered in the context of the domain of study, the data collection method, and the problem to be solved. How the data is cleaned will depend on all these things and more.\n",
    "- Often, there isn't one single \"correct\" way to clean a particular data set. The most important thing is to keep a copy of the \"messy\" data for reference, and to clearly document all of the data cleaning choices you made as well as why you made them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to import some necessary software\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Load the messy data\n",
    "\n",
    "- If you're working in the Haas Executive Education cloud server, you will need to upload your dataset. Go to [bee.haas.berkeley.edu)](bee.haas.berkeley.edu). Click on the \"haas-ds-online\" folder, then the \"data\" folder. Then, click the \"Upload\" button at the top right. The relative path to your dataset is now \"data/name-of-your-file\"\n",
    "- If you're on your computer, make sure you know the relative path of your data file. Putting it in the same directory as your notebook will help.\n",
    "- Replace the ... with the relative path of your data file. Don't forget the file extension.\n",
    "- Use the first code cell if your data is in a csv (comma separated values) file.  Use the second code cell if your data is in an Excel file.\n",
    "- If your data is in a different file type, you can see if there are functions to read it with Pandas at [this link](https://pandas.pydata.org/docs/user_guide/io.html). Note that csv and Excel files tend to be the easiest to work with in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For csv file data\n",
    "data = pd.read_csv(...)\n",
    "\n",
    "# show the first 5 rows of the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  For Excel file data\n",
    "data = pd.read_csv(...)\n",
    "\n",
    "# show the first 5 rows of the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Look at data types, missing values, distributions\n",
    "The following methods and attributes will work for most datasets and can help you find missing values and outliers that might need to be cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of rows and columns\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show summary statistics for numerical data\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the distributions of numerical data\n",
    "# the figsize parameter makes the histograms bigger\n",
    "data.hist(figsize=(14,10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a correlation matrix for all numerical columns of data\n",
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show how many null values are in each column\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the types of data in each column\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Handle missing values (if they exist)\n",
    "\n",
    "- the `fillna` method will fill all null/missing values with the value you put in the parentheses. You can also fill missing values using particular methods with the `method` argument.  Setting `method` to \"ffill\" will propagate the last valid observation forward to next valid, while \"bfill\" will use the next valid observation to fill the gap.\n",
    "- depending on your problem and dataset, you may want to fill missing values using an average value, a previous or subsequent value, or other imputation methods. \n",
    "- depending on your problem and dataset, you may want to drop missing values using `dropna`. The `axis` parameter specifies whether rows with missing data (axis=0) or columns with missing data (axis=1) are dropped.\n",
    "- note that both `fillna` and `dropna` do NOT overwrite your original DataFrame unless you set the `inplace` parameter to `True` or save the result of the expression to a variable as in the two cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only run this cell if you want to fill missing values\n",
    "data = data.fillna(value=None, method=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only run this cell if you want to drop missing values\n",
    "data = data.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Handle data type issues (if they exist)\n",
    "\n",
    "- generally, Scikit-Learn needs data to be numerical (int64 or float64)\n",
    "- sometimes, Pandas will read in numerical data as text, or a non-numerical value will appear in a numerical column (e.g. \"unknown\" in a column of ages)\n",
    "- non-numerical data can be dropped or converted to numerical data (through dummy or one-hot encoding, through imputation, or through forcing Panda to read in data as a certain type). The best option will always depend on the dataset, the domain, and the problem being solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode all categorical data in your dataset\n",
    "data = pd.get_dummies(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only columns with numerical data\n",
    "data = data.select_dtypes(\"number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a specific column into numeric data\n",
    "# may not work if there is text that Pandas can't easily interpret as a number\n",
    "pd.to_numeric(data[...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Perform other dataset-specific cleaning tasks\n",
    "\n",
    "This might include:\n",
    "- using array operations to convert units\n",
    "- feature engineering: creating new columns of numerical data from text data (or other numerical data)\n",
    "- dropping irrelevant rows or columns\n",
    "\n",
    "The [Official Pandas Library Cheat Sheet](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf) has a list of common operations as well as what they do. You can also look up details on Pandas functions by searching the [documentation](https://pandas.pydata.org/docs/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can do any other data cleaning here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save the cleaned dataset to a file\n",
    "Replace the ... with the path you want for your data file. For example, to save your data to a file called my_data.csv in the data folder, you would use the path \"data/my_data.csv\".\n",
    "\n",
    "To save the data to formats besides csv or Excel, please refer the [Pandas Input/Output documentation](https://pandas.pydata.org/docs/user_guide/io.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the cleaned dataset to a csv file\n",
    "data.to_csv(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the cleaned dataset to an Excel file\n",
    "data.to_excel(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "- Image credit: \"Cleaning02\", Nick Youngson / Alpha Stock Images. Licensed under [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook author: Keeley Takimoto"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
